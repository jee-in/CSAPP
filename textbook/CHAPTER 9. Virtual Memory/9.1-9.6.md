## 개관

시스템에 있는 프로세스들은 다른 프로세스들과 CPU와 메인 메모리를 공유한다. 메인 메모리를 공유하는 일은 몇몇 위기를 유발한다. CPU에 대한 요구가 증가하면 프로세스는 자연스럽게 느려진다. 너무 많은 프로세스들이 너무 많은 메모리를 필요로 한다면, 몇몇 프로세스들은 실행할 수도 없게 된다. 만약 프로그램이 메모리 공간을 차지하지 못하게 되면 상황이 안 좋아진다. 메모리가 오염에 취약해진다. 만약 어떤 프로세스가 의도적이지 않게 다른 프로세스가 사용 중인 메모리에 쓰기를 하게 된다면, 그 프로세스는 프로그램의 로직과 전혀 관계 없이 혼란스러운 방식으로 실패하게 된다.

메모리를 더 효율적으로, 에러 없이 관리하기 위해서 현대 시스템은 가상 메모리 virtual memory (VM)라고 알려져 있는 "메인 메모리의 추상화"를 제공한다. 가상 메모리는 하드웨어 예외들, 하드웨어 주소 번역, 메인 메모리, 디스크 파일, 커널 소프트웨어의 아름다운 상호작용이다. 가상 메모리는 각각의 프로세스들에게 크고, 통일되어 있고, 사적인 주소 공간을 제공한다. 하나의 정확한 메커니즘으로 가상 메모리는 세 가지 중요한 기능을 제공한다.   

(1) 가상 메모리는 메인 메모리를 디스크에 저장되어 있는 주소 공간(역주: 에 있는 데이터)에 대한 캐시로 활용함으로써 메인 메모리를 효율적으로 사용한다. 가상 메모리는 메인 메모리에 활성화된 영역만 있도록 관리하며, 필요할 때마다 데이터를 디스크와 메인 메모리 사이에서 왔다갔다 이동시킨다.  
(2) 가상 메모리는 각 프로세스에게 통일된 주소 공간을 제공하여 메모리 관리를 단순화한다.  
(3) 가상 메모리는 다른 프로세스들에 의해 각 프로세스의 주소 공간이 오염되는 것을 막아준다.  

가상 메모리는 컴퓨터 시스템에서 가장 위대한 아이디어 중 하나이다. 가상 메모리가 성공할 수 있었던 주된 이유는 가상 메모리가 응용 프로그램 개발자의 개입 없이도 조용하고 자동적으로 작동한다는 점 때문이다. 이렇게 가상 메모리가 뒤에서 알아서 잘 작동한다면, 왜 개발자가 가상 메모리를 이해해야 할까? 여기에는 몇 가지 이유가 있다.

- 가상 메모리는 핵심이다. 가상 메모리는 컴퓨터 시스템의 모든 레벨에 스며들어 있다. 가상 메모리는 하드웨어 예외, 어셈블러, 링커, 로더, 공유되는 오브젝트, 파일, 프로세스들의 설계에 있어서 핵심적인 역할을 한다. 가상 메모리를 이해하면 시스템이 전반적으로 어떻게 작동하는지 더 잘 이해할 수 있을 것이다.
- 가상 메모리는 강력하다. 가상 메모리는 응용 프로그램에게 메모리의 덩어리를 생성하고 파괴하는 능력, 디스크 파일의 부분과 메모리의 덩어리를 연결하는 능력, 다른 프로세스들과 메모리를 공유하는 능력을 제공한다. 예를 들어, 당신은 메모리 위치를 읽고 쓰는 것을 통해 디스크 파일의 내용물을 읽거나 변경할 수 있다는 사실을 아는가? 아니면 당신은 명시적인 복사 없이도 파일의 내용을 메모리로 로드할 수 있다는 것을 아는가? 가상 메모리를 이해하면 당신의 응용 프로그램에 가상 메모리의 강력한 기능들을 활용할 수 있을 것이다.
- 가상 메모리는 위험하다. 응용 프로그램은 응용 프로그램이 변수를 참조할 때마다, 포인터를 역참조할 때마다, malloc과 같은 동적 할당 패키지를 호출할 때마다 가상 메모리와 상호작용한다. 만약에 가상 메모리가 적절하지 않게 사용된다면, 응용 프로그램은 당혹스럽고 은밀한 메모리 관련 버그를 겪을 수 있다. 예를 들어, 잘못된 포인터를 가진 프로그램은 "segmentation fault" 또는 "protection fault"와 함께 즉시 작동을 멈출 수 있다. 이때 프로그램이 작동을 멈추기 전까지 조용하게 몇 시간을 동작했을 수도 있고, 더 무서운 것은 부정확한 결과와 함께 실행을 완료했을 수도 있다. 가상 메모리를 이해하고 가상 메모리를 관리하는 malloc과 같은 할당 패키지를 이해하는 것은 이러한 에러를 피하는 데 도움을 줄 것이다.  

이 챕터는 가상 메모리를 두 가지 각도에서 바라본다. 이 챕터의 절반은 가상 메모리가 어떻게 동작하는지를 묘사한다. 이 챕터의 나머지 절반은 응용 프로그램에 의해 가상 메모리가 어떻게 사용되고 관리되는지를 묘사한다. VM이 복잡하다는 데에는 이견이 없다. 하지만 좋은 소식은 이 책의 자세한 내용까지 공부하고 난다면, 당신은 작은 시스템의 가상 메모리 메커니즘을 직접 따라해볼 수 있다는 것이다. 이렇게 해본다면 가상 메모리의 아이디어는 평생 확실하게 이해할 수 있을 것이다.  

이러한 이해를 기반으로 당신의 프로그램에서 가상 메모리를 어떻게 사용하고 관리해야 할지도 보여줄 것이다. 당신은 명시적 메모리 매핑을 통해 가상 메모리를 관리하는 방법을 배울 것이고, malloc 패키지와 같은 동적 저장소 할당기를 호출하는 방법을 배우게 될 것이다. 당신은 또한 C 프로그램에서 메모리 관련 에러를 일으키는 주범을 배울 것이고 이를 피할 수 있는 방법을 배울 것이다.  

## 9.1 물리 주소 방식과 가상 주소 방식
컴퓨터 시스템의 메인 메모리는 M개의 바이트 크기의 연속적인 셀의 배열로 구성되어 있다. 각각의 바이트는 고유한 물리 주소 physical address(PA)를 갖는다. 첫 번째 바이트는 주소 0을, 다음 바이트는 주소 1을, 그 다음 바이트는 주소 2를 가진다. 이 간단한 구조를 봤을 때 CPU가 메모리에 접근하는 가장 자연스러운 방식은 물리 주소를 사용하는 방법일 것이다. 우리는 이러한 접근을 물리 주소 방식 physical addressing이라고 한다. 사진 9.1은 물리 주소 4에서 시작하는 4바이트 워드를 읽어서 CPU에 로드하는 동작이 실행되는 상황에서의 물리 주소 방식을 보여준다. CPU가 로드 동작을 실행할 때 CPU는 유효한 물리 주소를 생성하고 이를 메모리 버스를 통해 메인 메모리에게 전달한다. 메인 메모리는 전달받은 물리 주소에서 시작하는 4 바이트 워드를 가져와서 CPU에게 전달한다. 그러면 CPU는 그 데이터를 레지스터에 저장한다.

디지털 신호 프로세서, 임베디드 마이크로컨트롤러와 같은 초기 PC들은 물리 주소 방식을 사용했다. 그리고 Clay 슈퍼 컴퓨터는 여전히 물리 주소 방식을 사용한다. 하지만 현대 프로세서들은 가상 주소 방식 virtual addressing이라고 알려진 형태의 주소 형식을 사용한다. 이는 사진 9.2에서 확인할 수 있다.  

가상 주소 방식에서 CPU는 가상 주소 virtual address(VA)를 생성하여 메인 메모리에 접근한다. 가상 주소는 메인 메모리에 보내지기 전에 적절한 물리 주소로 변환된다. 가상 주소를 물리 주소로 변환하는 작업은 주소 번역 address translation이라고 알려져 있다. 예외 처리할 때처럼, 주소 번역은 CPU 하드웨어와 운영 체제 사이의 긴밀한 협업을 요구한다. MMU (Memory Management Unit)라고 불리는 CPU 칩에 있는 전용 하드웨어는 그때그때 필요할 때마다 가상 주소를 번역한다. 이때 MMU는 메인 메모리에 저장되어 있는, 운영체제가 그 내용을 관리하는 조회 테이블을 사용한다.

## 9.2 주소 공간

주소 공간 address space는 음수가 아닌 정수 주소들의 정렬된 집합이다.  
만약 주소 공간에 있는 정수들이 연속적이라면, 우리는 그 주소 공간을 선형적 주소 공간이라고 말할 수 있다. 우리의 논의를 간단하게 만들기 위해서, 주소 공간은 항상 선형적 주소 공간을 가정하겠다. 가상 메모리가 있는 시스템에서, CPU는 가상 주소 공간 virtual address space라고 불리는 N개(2<sup>n</sup>) 크기인 주소 공간으로부터 가상 주소를 생성한다.   

주소 공간의 크기는 가장 큰 주소를 표현하는 데 필요한 비트 수에 의해 정해진다. 예를 들어, N개(2<sup>n</sup>)의 주소를 가진 가상 주소 공간은 n-비트 주소 공간이라고 불린다. 현대 시스템은 일반적으로 32비트나 64비트 가상 주소 공간을 지원한다. (역주:  32비트 주소 공간이란 2<sup>32</sup>(약 42억) 개의 가상 주소를 사용할 수 있음을 의미합니다.)
시스템은 M 바이트의 물리 주소에 대응되는 물리 주소 공간 physical address space도 가진다. M은 2의 제곱일 필요는 없지만, 논의의 편의를 위해서 M 또한 2의 제곱이라고 가정하겠다.  

주소 공간이라는 개념은 중요하다. 왜냐하면 주소 공간은 byte로 된 데이터 오브젝트 data object와 그것의 속성인 주소 이 둘을 분명하게 구별해주기 때문이다. 이러한 구별을 인식했다면, 우리는 각각의 데이터 오브젝트가 서로 다른 주소 공간인 가상 주소 공간과 물리 주소 공간에서 선택된 여러 개의 독립적인 주소, 즉 가상 주소와 물리 주소를 가질 수 있다는 것을 알 수 있다. 이것은 가상 메모리의 기본적인 아이디어이다. 메인 메모리의 각 바이트는 가상 주소 공간에서 선택된 가상 주소와 물리 주소 공간에서 선택된 물리 주소를 가진다.

## 9.3 캐싱을 위한 도구로서 가상 메모리

개념적으로, 가상 메모리는 디스크에 저장되어 있는 N개의 연속적인 바이트 크기의 셀의 배열로 구성되어 있다. (역주: 가상 메모리는 말 그대로 가상 메모리이다. 가상 메모리에 특정 데이터가 저장(사실은 디스크에 저장되어 있는 데이터를 물리 메모리에 캐싱)되기 전까지 가상 메모리는 메인 메모리의 용량을 차지하지 않는다.) 각각의 바이트는 고유한 가상 주소를 가진다. 이 가상 주소는 배열의 인덱스로서 기능한다. 디스크에 저장되어 있는 배열의 내용물은 메인 메모리에 캐싱된다. 다른 메모리 계층에서의 캐싱과 같이, 디스크에 있는 데이터 (낮은 레벨)는 블록으로 분할된다. 블록들은 디스크와 메인 메모리 (높은 레벨) 사이에서 전송 단위로서의 역할을 한다. 가상 메모리 시스템은 가상 메모리를 가상 페이지 virtual pages (VPs)라고 불리는 고정된 사이즈의 블록으로 분할한다. 각각의 가상 페이지는 P(2 <sup>p</sup>) 바이트 크기이다. 비슷하게, 물리 메모리도 물리 페이지 physical pages (PPs)로 분할된다. 이 또한 P 바이트 크기이다. (물리 페이지는 페이지 프레임 page frames이라고도 불린다.)

어느 시점에서나, 가상 페이지의 집합은 세 가지 (공통 집합을 가지지 않은) 하위 집합으로 분할된다.

- 미할당 Unallocated
    - 가상 메모리에 의해 아직 할당되지 않은 (또는 생성되지 않은) 페이지. 
    - 미할당된 블록은 데이터를 가지지 않는다. 따라서 디스크에서 아무 공간도 차지하지 않는다.
- 캐시됨 Cached
    - 할당된 페이지이면서 물리 메모리에 할당된 것
- 캐시되지 않음 Uncached
    - 할당된 페이지이면서 물리 메모리에 할당되지 않은 것

사진 9.3은 8개의 가상 페이지를 가진 작은 가상 메모리를 보여준다.
가상 페이지 0과 3은 아직 할당되지 않았다. 따라서 아직 디스크에 존재하지 않는다.
가상 페이지 1, 4, 6은 물리 메모리에 캐시되어 있다. 
가상 페이지 2, 5, 7은 할당되었지만 현재 물리 메모리에 캐시되지 않았다.

### 9.3.1 DRAM 캐시 구성

메모리 계층에서 서로 다른 캐시들을 구분하기 위해 우리는 CPU와 메인 메모리 사이에 있는 L1, L2, L3 캐시 메모리를 지칭할 때 SRAM 캐시라는 용어를 사용할 것이다. 그리고 메인 메모리에 있는 가상 페이지를 캐싱하는 가상 메모리 시스템의 캐시를 지칭할 때 DRAM 캐시라는 용어를 사용할 것이다.  

메모리 계층에서 DRAM 캐시의 위치는 그것이 구성된 방식에 큰 영향을 미친다.
DRAM이 SRAM보다 10배 더 느리다는 사실, 그리고 디스크가 DRAM보다 100,000배 더 느리다는 사실을 떠올려라. 따라서, DRAM 캐시에서의 손실은 SRAM 캐시에서의 손실과 비교했을 때 더 비싸다. 왜냐하면 DRAM 캐시의 손실은 디스크로부터 처리되는 반면, SRAM 캐시의 손실은 일반적으로 DRAM 기반의 메인 메모리로부터 처리되기 때문이다. (역주: 손실 miss란 CPU가 데이터를 캐시나 메모리에서 찾으려 했을 때, 해당 데이터가 그 계층에 없어서 실패하는 것을 의미합니다.)
더군다나 디스크 부문의 첫 번째 바이트를 읽어들이는 비용은 디스크에서 연속적인 바이트를 읽어들이는 것보다 100,000배 더 느리다.
핵심은 DRAM 캐시의 구성이 전적으로 엄청난 규모의 미스 비용으로 작동한다는 것이다.

첫 번째 바이트에 접근하는 비용과 큰 손실 비용 때문에 가상 페이지는 크기가 큰 편이다. 일반적으로 4KB에서 2MB이다.
큰 손실 비용 때문에 DRAM 캐시는 완전히 연관형 fully associative이다. 즉, 모든 가상 페이지는 아무 물리 페이지에 위치할 수 있다. 

손실에 대한 교체 정책도 중요한 역할을 하게 된다. 왜냐하면 잘못된 가상 페이지를 교체하는 것과 관련된 손실은 너무 크기 때문이다. 따라서 운영체제는 하드웨어가 SRAM 캐시를 위해 사용하는 것과 비교했을 때 DRAM 캐시를 위해서 조금 더 정교한 교체 알고리즘을 사용한다. 
(이러한 교체 알고리즘은 우리의 범위를 넘어선다.) 결론적으로 디스크에 접근하는 시간이 크기 때문에 DRAM 캐시는 항상 write-through 대신에 write-back을 사용한다.

### 9.3.2 페이지 테이블
다른 캐시와 마찬가지로, 가상 메모리 시스템은 가상 페이지가 DRAM에 캐시되어 있는지 확인해야 한다. 가상 페이지가 DRAM 어딘가에 캐시되어 있다면 시스템은 그것이 어떤 물리 페이지(역주: 메인 메모리)에 캐시되었는지 알아내야 한다. 만약 손실 miss이 있다면, 시스템은 디스크에서 가상 페이지가 어디에 저장되어 있는지 알아내고, 물리 메모리(역주: 메인 메모리)에서 희생 페이지를 찾고, 디스크로부터 DRAM으로 가상 페이지를 복사하여 희생 페이지를 대체해야 한다.

이러한 능력은 운영체제 소프트웨어, MMU 안에 있는 주소 번역 하드웨어 그리고 페이지 테이블(가상 페이지와 물리 페이지를 매핑하는, 물리 메모리(역주: 메인 메모리)에 저장되어 있는 자료 구조)의 조합이 제공한다. 주소 번역 하드웨어는 가상 주소를 물리 주소로 변환해야 할 때마다 페이지 테이블을 읽는다. 운영체제는 페이지 테이블의 내용을 유지하고 디스크와 DRAM 사이에서 페이지를 왔다갔다 옮기는 일을 한다.

사진 9.4는 페이지 테이블의 기본적인 구성을 보여준다. 페이지 테이블은 페이지 테이블 항목 page table entries (PTEs)의 배열이다. 가상 주소 공간에 있는 각 페이지는 페이지 테이블에서 고정 오프셋 fixed offset 위치에서 PTE를 가진다. 편의를 위해서 각각의 PTE가 유효한 비트와 n개의 비트로 이루어진 주소 필드로 이루어졌다고 가정하겠다. 유효한 비트는 가상 페이지가 현재 DRAM에 캐시되었는지 아닌지를 나타낸다. 만약 유효한 비트가 설정되었다면, 주소 필드는 가상 페이지가 캐시되어 있는, DRAM의 물리 페이지의 시작을 가리킨다. 만약 유효 주소가 설정되어 있지 않다면, null 주소는 가상 페이지가 아직 할당되지 않았음을 가리킨다. 아니면 주소가 디스크에 있는 가상 페이지의 시작을 가리킨다.

사진 9.4에 있는 예시는 8개의 가상 페이지와 4개의 물리 페이지를 가진 시스템의 페이지 테이블을 보여준다. 4개의 가상 페이지(VP 1, VP 2, VP 4, VP 7)는 현재 DRAM에 캐시되어 있다. 2개의 페이지(VP 0, VP 5)는 아직 할당되지 않았다. 나머지 (VP 3, VP 6)는 할당되기는 했지만 현재 캐시되지 않았다. 사진 9.4에서 중요한 점은 DRAM 캐시가 완전히 연관형 fully associative이기 때문에 모든 물리 주소가 모든 가상 페이지를 보유할 수 있다는 점이다.

### 9.3.3 페이지 적중 Page Hits

CPU가 DRAM에 캐시되어 있는 VP 2 (가상 페이지 2)에 있는 워드 데이터를 읽을 때 어떤 일이 일어나는지 생각해 보라. 9.6에서 자세하게 설명할 기술을 사용하여, 주소 번역 하드웨어는 PTE 2 (페이지 테이블 항목 2)의 위치를 찾기 위해 가상 주소를 인덱스처럼 사용한다. PTE에서 유효 비트가 설정되어 있다면, 주소 번역 하드웨어는 VP 2가 메모리에 캐시되어 있다는 사실을 알게 된다. 주소 번역 하드웨어는 데이터의 물리 주소를 구성할 때 PTE에 있는 물리 주소를 사용한다. (역주: 읽으려고 하는 데이터가 DRAM 물리 주소에 캐시되어 있는 경우 페이지 적중 page hit를 했다고 말한다.)

### 9.3.4 페이지 오류 Page Faults

가상 메모리의 관점에서, DRAM 캐시 손실 misss는 페이지 오류 page fault라고 알려져 있다. 사진 9.6은 우리의 예시 테이블의 오류가 나기 전 상태를 보여준다. CPU는 DRAM에 캐시되지 않은 VP 3에 있는 데이터를 참조한다. 주소 번역 하드웨어는 메모리로부터 PTE 3을 읽고, VP 3이 캐시되어 있지 않는다는 사실을 유효 비트로부터 추론한다. (역주: PTE 3에는 가상 주소를 통해 접근한다.) 이러한 상황에서 주소 번역 하드웨어는 페이지 폴트 예외 page fault exception을 발생시킨다. page fault exception은 커널에서 페이지 폴트 예외 핸들러 page fault exception handler를 일으키는데, 이때 이 핸들러는 희생 페이지를 고른다. (우리의 예시에서는 물리 주소 3에 저장되어 있는 가상 페이지 4이다.) 만약 VP 4가 변경된 적이 있다면, 커널은 VP 4를 디스크로 다시 복사시킨다. 어떤 경우이든지 커널은 VP 4가 더 이상 메인 메모리에 저장되어 있지 않다는 사실을 반영하기 위하여 VP 4에 대한 페이지 테이블 항목 PTE를 변경시킨다. (역주: PTE 4의 유효 비트를 1에서 0으로 변경시킨다는 의미이다.)  

그 다음에 커널은 가상 페이지 3을 디스크로부터 메모리의 물리 주소 3으로 복사하여, 페이지 테이블 3을 업데이트하고 수행을 종료한다. 핸들러가 수행을 종료하면 return, 그것은 faulting 작업을 재시작한다. 이 작업은 faulting 가상 주소를 주소 번역 하드웨어에 다시 전송한다.
하지만 이제, 가상 페이지 3은 메인 메모리에 케시되었고 페이지 적중은 주소 번역 하드웨어에 의해 정상적으로 처리된다. 사진 9.7은 page fault 이후의 우리의 예시 페이지 테이블의 상태를 보여준다.  

가상 메모리는 초기 1960년대에 발명되었다. 이 당시는 커지는 CPU 메모리의 갭이 SRAM 캐시를 등장하게 만들기 오래 전이다.???

결과적으로 가상 메모리 시스템은 SRAM 캐시와 많은 아이디어가 비슷함에도 SRAM 캐시와 다른 용어를 사용하게 되었다. 가상 메모리의 관점에서 블록은 pages로 알려져 있다. 디스크와 메모리 사이에서 페이지를 옮기는 일은 스와핑 swappiing 또는 페이징 paging이라고 알려져 있다. 페이지는 디스크에서 DRAM으로 스와핑되어 들어오고 swapped in (paged in), DRAM에서 디스크로 스와핑되어 나간다 swapped out(paged out). 손실 miss가 발생했을 때 페이지를 swap in 하기 전 마지막 순간까지 기다리는 전략은 요구 페이징 demand paging이라고 알려져 있다. 손실 miss를 예측해서 페이지가 실제로 참조되기 전에 페이지를 스왑하려고 시도하는 등의 다른 방법도 가능하다. 핮히만 모든 현대 시스템은 demand paging을 사용한다.

페이지 오류 핸들러는 가상 페이지 4를 희생 페이지로 선택하고, 디스크에 있는 가상 페이지 3을 복사하여 희생 페이지 자리에 붙여넣는다. 페이지 오류 핸들러가 faulting 작업을 재시작하고 나서는 핸들러는 예외를 발생시키지 않고 메모리로부터 데이터 워드를 정상적으로 읽을 것이다.  

### 9.3.5 페이지의 할당 Allocation PAges

- 사진 9.8 설명  
새로운 가상 페이지 할당하기
커널은 디스크의 가상 페이지 5를 할당하고 PTE 4가 새로운 위치를 가리키도록 만든다.

사진 9.8은 운영체제가 가상 메모리의 새로운 페이지를 할당했을 때 (예를 들어, malloc을 호출한 결과) 우리의 예시 페이지 테이블에 어떤 영향이 생기는지를 보여준다. 예시에서, 가상 페이지 5는 디스크에 공간을 할당하고 PTE 5가 디스크에 새롭게 생성된 페이지를 가리키도록 업데이트한다.

### 9.3.6 문제 해결을 위한 지역성 Locality to the Rescue Again

가상 메모리의 아이디어를 배우게 되면 가상 메모리가 심각하게 비효율적일 것이라는 인상을 받을 수 있다. 큰 미스 비용 miss penalties를 생각하면 우리는 페이징이 프로그램의 성능을 파괴시킬 것이라는 걱정을 하게 된다. 하지만 실제에서는 지역성 locality(역주: 최근 참조된 데이터는 앞으로도 참조될 가능성이 높다.) 덕분에 메모리는 잘 작동한다.

프로그램이 실행되는 동안 프로그램이 참조하는 서로 다른 페이지의 총 개수가 물리 메모리의 총 용량을 초과할지라도 지역성의 원칙에 의해서 어떤 시점에서든지 프로그램은 작업 집합 working set 또는 거주 집합 resident set이라고 알려진 활성화 페이지 active page의 작은 집합에서 일한다. 
작업 집합이 메모리로 paged in 될 때의 초기 비용 그 이후에 이루어지는 작업 집합에 대한 참조는 부가적인 디스크에서의 이동 필요 없이 페이지 적중 hit이 된다. 

우리의 프로그램이 좋은 잠정 구역성 temporal locality를 가지고 있는 한, 가상 메모리 시스템은 잘 작동한다. 하지만 물론, 모든 프로그램이 좋은 temporal locality를 보여주는 것은 아니다. 만약 working set의 사이즈가 물리 메모리의 사이즈를 초과한다면, 프로그램은 스래싱 thrashing이라고 알려진 불행한 상황을 낳을 수도 있다. 쓰래싱은 페이지가 스왑되어 들어오고 스왑되어 나가는 일이 반복적으로 일어나는 것을 말한다. 비록 가상 메모리가 주로 효율적이기는 하지만, 만약 프로그램의 성능이 너무 느려진다면, 현명한 개발자는 쓰래싱의 가능성을 고려할 것이다.

> 페이지 폴트의 수와 그 외 많은 정보들은 Linux의 `getrusage` 함수를 이용하면 모니터링할 수 있다.

## 9.4 메모리 관리 도구로서 가상 메모리 VM as a Tool for Memory Management

이전 절에서, 더 큰 가상 주소 공간으로부터 페이지를 캐시하기 위해 가상 메모리가 DRAM을 사용할 때 메커니즘을 어떻게 제공하는지 살펴봤다. 흥미롭게도, DEC PDP-11/70과 같은 몇몇 초기 시스템은 실제 사용 가능한 물리 주소보다 작은 가상 주소 공간을 지원했다. 그럼에도 가상 메모리는 여전히 유용한 메커니즘이었다. 왜냐하면 가상 메모리는 메모리 관리를 매우 간단하게 만들고 메모리를 보호하는 자연스러운 방법을 제공했기 때문이다.  
지금까지 우리는 물리 주소에 단일 가상 주소 공간을 매핑하는 단일 페이지 테이블을 가정했다. 사실은 운영체제는 각 프로세스마다 별도의 페이지 테이블을, 따라서 별도의 가상 주소 공간을 제공한다. 사진 9.9는 기본적인 아이디어를 보여준다. 예에서, 프로세스 i를 위한 페이지 테이블은 가상 페이지 1을 물리 페이지 2에, 가상 페이지 2를 물리 페이지 7에 매핑한다. 마찬가지로, process j를 위한 페이지 테이블은 가상 페이지 1을 물리 주소 7에, 가상 주소 2를 물리 주소 10에 매핑한다. 여러 개의 가상 페이지가 하나의 공유된 물리 페이지에 매핑될 수 있음에 주목하라.  
페이징 요구 demand paging과 분리된 가상 주소 공간의 조합은 시스템에서 메모리가 사용되고 관리되는 방법에 중요한 영향을 미친다. 특히, 가상 메모리는 링킹 linking과 로딩 loading, 코드와 데이터의 공유 그리고 응용 프로그램에 메모리를 할당하는 일을 간단하게 만든다.

- 링킹을 간단하게 만든다. 분리된 주소 공간은 각 프로세스가 자신의 메모리 이미지를 위해 동일한 기본 형식을 사용하도록 한다. 물리 주소에서 코드와 데이터가 실제로 어디에 위치하는지와 관계없이 말이다. 예를 들어서, 사진 8.13에서 봤듯이, 리눅스 시스템에서 모든 프로세스는 비슷한 메모리 형식을 가진다. 64 비트 주소 공간에서, 코드 세그먼트 code segment는 항상 가상 주소 0x400000에서 시작한다.데이터 세그먼트는 코드 세그먼트 다음에 적당한 정렬 간격 alignment gap을 두고 뒤따른다. 스택은 사용자 프로세스 주소 공간에서 가장 높은 부분을 차지하고 아래 방향으로 자라난다. 이러한 통일성은 링커의 설계와 구현을 엄청나게 간단하게 만들어주고, 링커가 물리 메모리에서의 최종 위치에 독립적인 완전히 연결된 실행 가능 파일들을 생산할 수 있게 해준다.

- 로딩을 간단하게 해준다. 가상 메모리는 또한 실행 가능한 파일과 공유된 목적 파일을 메모리에 로드하는 일을 간단하게 해준다. 목적 파일의 `.text`와 `.data` 부문을 새롭게 생성된 프로세스에 로드하려면, 리눅스 로더는 코드와 데이터 세그먼트를 위한 가상 메모리를 할당하고 이들을 유효하지 않다고(캐시되지 않았다고) 표시하고 그들의 페이지 테이블 항목이 목적 파일의 적절한 위치를 가리키도록 만든다. 흥미로운 점은 로더가 실제로는 디스크에서 메모리로 아무 데이터도 복사하지 않는다는 것이다. 
데이터는 각 페이지가 처음 참조되었을 때 가상 메모리 시스템에 의해 자동으로 요청 시에 paged in된다. 각 페이지가 참조되는 것은 CPU가 명령을 가져올 때나 실행 중인 명령이 메모리 위치를 참조할 때이다.  
연속적인 가상 페이지를 임의의 파일에 있는 임의의 위치로 매핑하는 것의 개념은 메모리 매핑 memory mapping이라고 알려져 있다. 리눅스는     이라는 시스템 콜을 제공한다. mmap은 응용 프로그램이 자신의 메모리를 매핑하는 것을 가능하게 해준다. 우리는 응용 프로그램 레벨의 메모리 매핑을 9.8절에서 더 자세히 알아볼 것이다.  
- 공유를 간단하게 한다. 분리된 주소 공간은 사용자 프로세스와 운영체제가 메모리를 공유하는 것을 관리하는 일관적인 메커니즘을 운영체제에게 제공한다. 일반적으로, 각각의 프로세스는 자신만의 사적인 코드, 데이터, 힙 그리고 스택 영역이 있다. 이들은 다른 프로세스들과 공유되지 않는다. 이 경우, 운영 체제는 서로 다른 물리 페이지에 대응되는 가상 페이지를 매핑하는 페이지 테이블을 생성한다.  
하지만 어떤 경우에는 프로세스가 코드와 데이터를 공유하는 것이 바람직할 때도 있다. 예를 들어, 모든 프로세스는 동일한, 운영체제의 커널 코드를 호출해야 한다. 그리고 모든 C 프로그램은 printf 등 표준 C 라이브러리에 있는 통상적인 것을 호출한다. 각 프로세스에 커널과 표준 C 라이브러리의 분리된 사본을 포함하기보다, 서로 다른 프로세스에 있는 적절한 가상 페이지를 같은 물리 페이지에 매핑함으로써 다수의 프로세스가 하나의 코드를 공유할 수 있도록 운영체제가 관리할 수 있다.

- 메모리 할당을 간단하게 만든다. 가상 메모리는 사용자 프로세스에게 추가적인 메모리를 할당할 때 사용할 수 있는 간단한 메커니즘을 제공한다. 사용자 프로세스에서 실행 중인 프로그램이 추가적인 힙 공간을 요청했을 때 (예를 들어, malloc을 호출한 결과로), 운영체제는 적절한 숫자의, 예를 들어 k만큼의, 연속적인 가상 메모리 페이지를 할당한다. 그리고 운영체제는 그 가상 메모리 페이지를 물리 주소 여기저기에 위치해 있는 k개의 임의의 물리 페이지에 매핑한다. 페이지 테이블이 있기 때문에, 운영체제가 물리 주소의 k개의 연속적인 페이지를 찾아나설 필요는 없다. 페이지들은 물리 주소에서 랜덤하게 흩어져 있을 수 있다.

## 9.5 메모리 보호를 위한 도구로서 가상 메모리 VM as a Tool for Memory Protection

현대의 모든 컴퓨터 시스템은 운영체제가 메모리 시스템에 접근할 수 있는 방법을 제공해야 한다. 사용자 프로세스가 자신의 읽기 전용 코드 섹션을 변경할 수 없도록 통제되어야 한다. 또한 사용자 프로세스는 커널에 있는 모든 코드와 자료 구조를 읽거나 변경할 수 없도록 통제되어야 한다. 사용자 프로세스는 다른 프로세스의 사적 메모리를 읽거나 쓸 수 없도록 관리되어야 하며, (명시적인 프로세스 간 통신 시스템 콜을 통해) 다른 프로세스들이 명시적으로 허용하지 않는 이상 다른 프로세스들과 공유되고 있는 모든 가상 페이지를 변경할 수 없도록 제한되어야 한다.   
우리가 이미 봤듯이, 분리된 가상 주소 공간을 제공하면 서로 다른 프로세스의 사적 메모리를 쉽게 격리시킬 수 있다. 하지만 주소 번역 메커니즘은 더 정교한 접근 통제를 제공하기 위해 자연스러운 방법으로 확장될 수 있다. CPU가 주소를 생성할 때마다 주소 번역 하드웨어가 PTE를 읽기 때문에, PTE에 몇몇 추가적인 승인 비트를 추가함으로써 가상 페이지의 내용물에 대한 접근을 간단하게 통제할 수 있다. 사진 9.10은 이러한 일반적인 아이디어를 보여준다.  
이 예에서, 우리는 각각의 PTE에 3개의 승인 비트를 추가했다. SUP 비트는 해당 페이지에 접근하기 위해서 프로세스가 커널 (관리자) 모드에서 실행되어야 하는지 아닌지를 가리킨다. 커널 모드에서 실행되는 프로세스들은 아무 페이지에 접근할 수 있지만, 사용자 모드에서 실행 중인 프로세스는 SUP이 0인 페이지에만 접근할 수 있다. READ와 WRITE는 해당 페이지에 대한 읽기와 쓰기 접근을 통제한다. 예를 들어, 만약 프로세스 i가 유저 모드에서 실행되고 있다면, 그것은 VP 0을 읽고, VP 1을 읽거나 그곳에 쓸 수 있는 권한을 가진 것이다. 하지만 VP 2에 접근하는 것은 허용되지 않는다.  
만약 명령이 이러한 권한을 위반한다면, CPU는 보호 오류 protection fault를 발생시킨다. 보호 오류는 커널에 있는 예외 처리 핸들러 exception handler에게 통제권을 넘긴다. 예외 처리 핸들러는 허가를 위반한 프로세스에게 SIGSEGV 시그널을 보낸다. 리눅스 쉘은 일반적으로 이러한 예외를 "segmentation fault" 세그멘테이션 오류라고 보고한다.

## 9.6 주소 번역

이 절은 주소 번역의 기초를 다룬다. 우리의 목표는 충분한 설명을 제공하여 당신이 몇몇 구체적인 예를 직접 수행해볼 수 있을 정도로, 가상 메모리를 지원하는 하드웨어를 깊이 이해할 수 있게 하는 것이다. 하지만 특히 timing과 관련하여 (이는 하드웨어 설계자에게 중요한 문제이지만 우리의 범위를 넘어서는 주제이다.) 우리가 많은 상세 내용을 생략하고 있다는 것은 기억해야 한다. 표 9.11에 이 절에서 사용할 예정인 용어들을 요약해두었으니 참고하기 바란다. 

- 기본 파라미터
    - 가상 주소 공간에 있는 주소의 개수
    - 물리 주소 공간에 있는 주소의 개수
    - 페이지 사이즈 (바이트)
- 가상 주소의 구성 요소
    - 가상 페이지 오프셋 (바이트)
    - 가상 페이지 넘버
    - TLB 인덱스
    - TLB 태그
- 물리 주소의 구성 요소
    - 물리 페이지 오프셋 (바이트)
    - 물리 페이지 넘버
    - 캐시 블록에 있는 바이트 오프셋
    - 캐시 인덱스
    - 캐시 태그

형식적으로 주소 번역은 N개의 요소로 이루어진 가상 주소 공간과 M개의 요소로 이루어진 물리 주소 공간 사이의 매핑을 말하는 것이다.

MAP: VAS -> PAS ∪ ∅
(Virtual Address Space) (Physical Address Space)

- A': 가상 주소에 데이터가 있는 경우. A는 물리 주소에 존재한다. A'는 물리 주소 공간에 있다.
- ∅: 가상 주소에 데이터가 있는 경우. A는 물리 주소에 존재하지 않는다.

사진 9.12는 이러한 매핑을 수행하기 위해 MMU가 페이지 테이블을 어떻게 이용하는지를 보여준다. CPU에 있는 컨트롤 레지스터인 페이지 테이블 베이스 레지스터 (PTBR)는 현재 페이지 테이블을 가리킨다. n개의 비트로 이루어진 가상 주소는 두 개의 구성 요소를 가진다. p개의 비트로 된 가상 페이지 오프셋 (VPO)와 (n-p) 개의 비트로 된 가상 페이지 넘버 (VPN)이다. MMU는 적절한 PTE를 선택하기 위해 VPN을 이용한다. 예를 들어, VPN 0은 PTE 0을 선택하고, VPN 1은 PTE 1을 선택한다. 대응되는 물리 주소는 페이지 테이블 항목에 있는 물리 페이지 넘버 (PPN)와 가상 주소에 있는 VPO의 결합이다. 물리 페이지와 가상 페이지가 둘 다 P 바이트 크기이기 때문에 물리 페이지 오프셋 (PPO)은 VPO와 동일하다는 점에 주목해라.  

사진 9.13(a)은 페이지 적중 page hit이 있을 때 CPU 하드웨어가 수행하는 단계들을 보여준다.

단계 1: 프로세스는 가상 주소를 생성하고 그것을 MMU에 전송한다.
단계 2: MMU는 PTE 주소를 생성하여 캐시/메인 메모리에게 PTE 요청한다.
단계 3: 캐시/메인 메모리는 MMU에게 PTE를 반환한다.
단계 4: MMU는 물리 주소를 만들고 그것을 캐시/메인 메모리에게 전송한다.
단계 5: 캐시/메인 메모리는 프로세서에게 요청된 데이터 워드를 반환한다.

하드웨어에 의해 전적으로 처리되는 페이지 적중 page hit과 다르게, 페이지 오류 page fault를 처리하는 일은 하드웨어와 운영 체제 커널 사이의 협업을 요구한다.

단계 1-3: 사진 9.13(a)에 있는 단계 1-3과 같다.
단계 4: PTE에 있는 유효 비트는 0이다. 따라서 MMU는 예외를 발생시킨다. 예외는 CPU에 있는 통제권을 운영 체제 커널에 있는 페이지 오류 예외 처리 핸들러에게 넘긴다.
단계 5: 오류 핸들러는 물리 메모리에서 희생 페이지를 찾는다. 만약 페이지가 변경된 적이 있다면 디스크에서 page out한다.
단계 6: 오류 핸들러는 새로운 페이지에 page in하고 메모리에 있는 PTE를 업데이트한다.
단계 7: 오류 핸들러는 원래의 프로세스로 돌아가서 faulting instruction이 재시작하게 만든다. CPU는 MMU에 위반하는(?) 가상 주소를 다시 보낸다. 가상 페이지가 이제 물리 주소에 캐시되어 있기 때문에 hit가 있다. 그리고 MMU가 사진 9.13(a)에 있는 단계들을 수행하고 나면 메인 메모리는 프로세서에게 요청된 워드를 반환한다.

### 9.6.1 캐시와 가상 메모리 통합하기

가상 메모리와 SRAM 캐시를 둘 다 이용하는 모든 시스템에서는 이슈가 있다. SRAM 캐시에 접근할 때 가상 주소를 사용할지 물리 주소를 사용할지 하는 이슈이다. trade-offs에 대한 자세한 논의는 우리의 범위를 넘어서지만, 대부분의 시스템은 물리 주소 방식을 선택하고 있다. 물리 주소 방식을 사용하면 다수의 프로세스가 캐시의 블록을 동시에 가지는 것 그리고 같은 가상 페이지로부터 블록을 공유하는 것이 간단해진다. 더군다나 접근 권한이 주소 번역 과정 중에 확인이 되기 때문에 캐시가 보호 이슈를 다룰 필요가 없어진다.  
사진 9.14는 물리적으로 주소를 얻은 캐시가 가상 메모리와 통합될 수 있음을 보여준다. 주요한 아이디어는 주소 번역이 캐시를 보기도 전에 발생한다는 것이다. 다른 데이터 워드를 캐시할 수 있는 것처럼 페이지 테이블 항목이 캐시될 수 있다는 것을 기억하라.

### 9.6.2 TLB를 이용하여 주소 번역의 속도 높이기

우리가 봤던 것처럼 CPU가 가상 주소를 생성할 때마다 MMU는 가상 주소를 물리 주소로 번역하기 위해 PTE를 가리켜야 한다. 가장 최악의 경우에 이는 메모리로부터 추가적인 fetch 작업을 요구한다. 이는 수십에서 수백 사이클의 비용이 드는 작업이다. 만약 PTE가 L1에 캐시되어 있다면 비용은 몇 안 되는 사이클로 줄어들 것이다. 하지만 많은 시스템은 MMU에 PTE의 작은 캐시를 포함함으로써 이러한 비용조차 제거하기 위해 노력한다. 이러한 캐시는 변환 색인 버퍼 Translation Lookaside Buffer (TLB)라고 불린다.  
TLB는 작고, 가상 주소로 접근되는 캐시이다. TLB의 각 라인(역주: 캐시 라인)이 하나의 PTE 블록을 저장하고 있다. TLB는 일반적으로 높은 정도의 associativity를 가진다. 사진 9.15에서 보여지는 것처럼, 인덱스와 태그 필드는 set 선택에 사용된다. (역주: 인덱스로 선택된 set 내에는 여러 개의 엔트리가 있을 수 있다. 이때의 엔트리가 곧 캐시 라인, PTE 블록을 의미한다.)인덱스와 태그 필드는 가상 주소에 있는 가상 페이지 넘버로부터 추출된다. 만약 TLB가 T = 2<sup>t</sup> 세트를 가졌다면, TLB 인덱스 (TLBI)는 VPN의 최소 t개의 의미 있는 significant 비트로 이루어져 있다. 그리고 TLB 태그 (TLBT)는 VPN에 있는 남아 있는 비트로 구성되어 있다.  

사진 9.16(a)는 TLB 적중 TLB hit가 있을 때 (일반적인 경우이다.) 일어나는 단계들을 보여준다. 여기서 핵심은 모든 주소 번역 단계가 CPU 칩 안에 있는 MMU에서 일어나고 그래서 빠르게 이루어진다는 것이다.  

단계 1: CPU는 가상 주소를 생성한다.  
단계 2-3: MMU는 TLB로부터 적절한 PTE를 가져온다.  
단계 4: MMU는 가상 주소를 물리 주소로 번역하고 물리 주소를 캐시/메인 메모리에 전송한다.  
단계 5: 캐시/메인 메모리는 요청받은 데이터를 CPU에게 전달한다.  

사진 9.16(b)에서 확인할 수 있듯이 TLB 손실 miss가 있으면, MMU는 L1 캐시로부터 PTE를 가져와야 한다. 새롭게 가져와진 PTE는 TLB에 저장된다. 이는 원래 존재하던 엔트리를 덮어쓸 수도 있다.


### 9.6.3 다층 페이지 테이블 Multi-Level Page Tables

아직까지, 우리는 시스템이 주소 번역을 위해 하나의 페이지 테이블을 이용하는 상황만을 가정하였다. 하지만 만약 우리가 32 비트 주소 공간, 4KB 페이지, 그리고 4 바이트 PTE를 가졌다면 우리는 4MB 용량의 페이지 테이블을 항상 메모리에 거주시켜야 할 것이다. (역주: 가상 주소 공간 2<sup>32</sup>은 4KB(= 2<sup>12</sup> 바이트) 크기의 페이지를 2<sup>20</sup>개 사용할 수 있다. (2<sup>12</sup> * 2<sup>20</sup> = 2<sup>32</sup>) 각 페이지에 대해 캐시 여부 및 물리 주소를 저장하고 있는 페이지 테이블의 한 항목이 4byte 크기라면 전체 페이지 테이블의 용량은 4(2<sup>2</sup>) * 2<sup>20</sup> = 2<sup>22</sup> = 4KB이다.) 응용 프로그램이 가상 주소 공간의 매우 작은 덩어리만 참조하고 있다고 하더라도 페이지 테이블은 똑같이 4KB 만큼의 메모리를 차지하게 된다. 64비트 주소 공간을 가진 시스템에서는 이러한 문제가 더 악화된다.  
페이지 테이블을 작게 만들기 위한 일반적인 접근은 하나의 페이지 테이블을 사용하는 대신에 계층별로 페이지 테이블을 사용하는 것이다. 구체적인 예시를 보면 이러한 아이디어가 잘 이해될 것이다. 32 비트 가상 주소 공간이 있고, 이 가상 주소 공간이 4KB 페이지로 구획되어 있고 (역주: 그렇다면 페이지 수는 2<sup>20</sup>개이다.), 페이지 테이블 항목이 각각 4 바이트인 상황을 생각해 보자. 이때 가상 주소 공간은 다음과 같은 형식을 가졌다고 가정해보자.  
- 처음 2K(2 * 2<sup>10</sup>) 개의 페이지는 코드와 데이터를 위해 할당되었다. 
- 다음 6K(6 * 2<sup>10</sup>) 개의 페이지는 할당되지 않았다.
- 다음 1K - 1(1 * 2<sup>10</sup> - 1) 개의 페이지 또한 할당되지 않았다.
- 마지막 1개의 페이지는 사용자 스택을 위해 할당되었다.  

사진 9.17은 가상 주소 공간을 위해 어떻게 2 레벨의 페이지 테이블 계층을 구성할 수 있을지를 보여준다.  

레벨 1 테이블에 있는 각각의 PTE는 가상 주소 공간의 4MB 덩어리를 매핑한다. 가상 주소 공간에서 각각의 덩어리는 1,024 개의 연속적인 바이트로 이루어져 있다. 예를 들어, PTE 0이 첫 번째 덩어리를 매핑하고, PTE 1이 그 다음 덩어리를 매핑한다. 가상 주소 공간이 4GB라는 것을 생각할 때, 1,024 개의 PTE는 전체 공간을 포함하기 충분하다. (역주: 4MB * 1,024 = 2<sup>2</sup> * 2<sup>20</sup> * 2<sup>10</sup> = 2<sup>32</sup> 바이트 = 4GB이다. 이는 전체 가상 메모리 주소의 크기이다.)  
만약 i라는 덩어리에 있는 모든 페이지가 미할당되었다면, 레벨 1의 PTE는 null이 된다. 예를 들어, 사진 9.17에 있는 덩어리 2-7은 미할당되었다. 하지만 만약 덩어리 1에 있는 페이지 중에 하나라도 할당되었다면, 레벨 1의 PTE i는 레벨 2의 페이지 테이블의 base를 가리킨다. 예를 들어, 사진 9.17에서, 덩어리 0, 1, 8의 모든 페이지가 또는 몇몇 페이지가 할당되었다면 이들의 레벨 1의 PTE들은 레벨 2의 페이지 테이블을 가리킨다.  
레벨 2에 있는 각각의 PTE는 가상 메모리의 4KB 페이지를 매핑한다. 이는 전에 살펴봤던 단일 페이지 테이블에서와 같다. 4 바이트 크기의 PTE를 가진 레벨 1과 레벨 2의 페이지 테이블은 모두 4KB (역주: 2<sup>2</sup> * 2<sup>10</sup>)로, 편리하게도 페이지와 크기가 같다.
